---
title: "Data Science on Unstructured Text Data - Quiz 3"
author: "Tamas Koncz"
date: '2018-02-22'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r, message=FALSE, include=FALSE}
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(tidytext)
library(gutenbergr)
library(topicmodels)
library(stringr)

library(dplyr)
library(ggplot2)
library(gridExtra)
```

#### 1. In your own words describe LDA  

I look at LDA as the cluster analysis for textual documents.  
In essence, it treats documents as being a mix of different (not too many) topics, and these topics are in turn a mix of words. It helps us find (~"cluster") the descriptive words for a topic, and the descriptive words for a topic.  
  

#### 2. In your own words, describe the process of a full tidy text analysis  
  
First step is unnesting tokens (words/ngrams) from the original text - creating a 1 token /row dataset. This can be massaged further (e.g. applying stopwords) to arrive at a tidy text format for our analysis.  
Analysis is generally based on some type of word-count-summary analysis, either relative (TF-IDF) or absolute  (sentiment analysis).  
Analysis is an iterative process - there can be always reasons to step back, and rerun results (e.g. adding new stop words that were uncovered).  
As a good practice, last step should be visualizing results for the ease of understanding.  
  
#### 3. Do a short tidy text analysis where you extract topics, explain why they are good or bad.  
  
I'll use the same books from quiz 2, Arthur Conan Doyle's "The Adventures of Sherlock Holmes" and Victor Hugo's Les Miserables.
```{r}
arthur_conan_doyle <- gutenberg_download(1661)
victor_hugo <- gutenberg_download(48731:48735)
```
  
  
  
Unnesting words:
```{r}
data(stop_words)
stop_words <- rbind(stop_words, data.frame(word = "de", lexicon = "custom"))

tidy_acd_books <- arthur_conan_doyle %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(author = "ACD") %>%
  mutate(book = "The Adventures of Sherlock Holmes")


tidy_vh_books <- victor_hugo %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(author = "VH") %>%
  mutate(book = "Les Miserables")
```
  
  
LDA needs a DocumentTermMatrix for an imput, so let's create one:  

```{r}
books_td <- bind_rows(tidy_acd_books, tidy_vh_books) %>%
              count(book, word) %>%
              arrange(desc(n))

books_dtm <- books_td %>%
              cast_dtm(book, word, n)
```

Running LDA analysis with 2 topics (seed = 93, for reproducible results):  
```{r}
books_lda <- books_dtm %>%
                LDA(k = 2, control = list(seed = 93))
```
  
  
  
We can visualize per-topic-per-word probabilities (top10 for both):
```{r}
books_topics <- tidy(books_lda, matrix = "beta")

books_top_terms <- books_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

books_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(title= "Words with biggest probability of belonging to a topic")
```
The above results make sense. Names (which are very unique to the books in this case, so the algorithm had an easy job) more-or-less match up with other specific words.  
Note the overlap in "day" - this is normal for LDA, as the overlap for textual document terms is not suprising - something that makes it different to regular clustering.  
  
It's also possible to visualize the words with the biggest probability difference in belonging to one topic or the other:  

```{r}
beta_spread <- books_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

top5 <- beta_spread %>%
  top_n(10, log_ratio)
bottom5 <- beta_spread %>%
  top_n(-10, log_ratio)

bind_rows(top5, bottom5) %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(x=term, y=log_ratio)) + 
    geom_bar(stat="identity") +
    coord_flip() +
    labs(title= "Words with biggest probability differences",
         subtitle = "As captured by the log(prob_topic1/prob_topic2) ratio")
```

  
Again, results make sense - there is not much challange here, as the names dominate (they were not removed from the dataset), so this is rather a demonstration of the algorithm's power.  
  
An intersting extension of this analysis would be to run the analysis on the twelve short stories of "The Adventures of Sherlock Holmes", and see how much it is able to pick up the separate storylines, when much of the people, places, etc. are shared.  
  
If I update the document to contain that analysis as well, it will be marked clearly with an updated date.


