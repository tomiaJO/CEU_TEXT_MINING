---
title: "Data Science on Unstructured Text Data - Quiz 2"
author: "Tamas Koncz"
date: '2018-02-19'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r, message=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)

library(dplyr)
library(ggplot2)
library(gridExtra)

```


#### 1. Explain in your words what the unnest_token function does  
  
Given a data.frame, it splits up a define column into pieces - for our purpose of text mining, it will break a string vector into words. (Instead of words, it could also do ngrams, sentences etc.) The result will be organized in the long format, with not-impacted column data being replicated for each row.  
  

#### 2. Explain your words what the gutenbergr package does  

gutenbergr is an R API for the public domain collection of Project Gutenberg (~centralizing all books no longer under copyright restrictions). gutenbergr data also includes information of the works, and text is likely stripped of unncessary parts (e.g. page number).  

#### 3. Explain in your words how sentiment lexicon work  

Sentiment lexicons are just mapping files for words (or ngrams, combination of words) to a respective sentiment score. This score can be of different granularity - just positive/negative, or a number scale, or even categorical (e.g. "anger/joy/...").  

#### 4. How does inner_join provide sentiment analysis functionality  
  
We connect our text dataset (assuming it's already tidy) to the sentiment lexicon by inner_join - which will result in only the words that are in both datasets. Now, we have a sentiment score for all the remaining words in our text.  

#### 5. Explain in your words what tf-idf does  
  
TF-IDF creates a score for how important a word is in a document. Technically, it is based on weighting the word by the times it appears, divided by a term for it's general frequency. (So words that appear many times, but are very general, will get lower scores, while "special" word to the text will have higher scores)  

#### 6. Explain why you may want to do tokenization by bigram  
  
Sometimes looking at individual words could be misleading to understand the whole document - a good example is to think of negating word combinations, like "That was not smart", which might seem a positive statement if we score it only based on single words, as "smart" is likely scored positively.  

#### 7. Pick two or more authors that you are familiar with, download their texts using the gutenbergr package, and do a basic analysis of word frequencies and TF-IDF  
  
  
First step is to just get the ids of books for two authors:
```{r}
arthur_conan_doyle_ids <- gutenberg_works(author == "Doyle, Arthur Conan")[["gutenberg_id"]]

victor_hugo_ids <- gutenberg_works(author == "Hugo, Victor" & language == "en")[["gutenberg_id"]]
```
  
  
Then proceeding to download the respective books:
```{r}
arthur_conan_doyle <- gutenberg_download(arthur_conan_doyle_ids)
victor_hugo <- gutenberg_download(victor_hugo_ids)
```
  
  
In the next step two things are combined: unnesting the text column to words, and in the same step removing the useless ones (stopwords).
_Note: "de" was added to the stop words list based on its frequency in Hugo's works, as likely it just appears in French names without any added meaning_
```{r}
data(stop_words)
stop_words <- rbind(stop_words, data.frame(word = "de", lexicon = "custom"))

tidy_acd_books <- arthur_conan_doyle %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)


tidy_vh_books <- victor_hugo %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```
  
  
Let's visualize the most common words for both authors ( _note: I checked before, words we removed in the antijoin step earlier indeed looked like garbage for the purposes of text analysis_ )
```{r, fig.width=12, fig.height=6, fig.align='center'}
p1 <- tidy_acd_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 2500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() + 
  labs(title = "Most frequent words in the works of Arthur Conan Doyle", 
       subtitle = "Filtered for words appearing at least 2500 times,\nafter removing stopwords")

p2 <- tidy_vh_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 1250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() + 
  labs(title = "Most frequent words in the works of Victor Hugo", 
       subtitle = "Filtered for words appearing at least 1250 times,\nafter removing stopwords")

grid.arrange(p1, p2, ncol = 2)
```
  
    
Maybe not suprisingly, many of the most frequent words are shared among the two authors - think of "time", "eyes", etc. We'll explore commonalities further in next steps.   
  
```{r}
frequency <- bind_rows(mutate(tidy_acd_books, author = "Arthur Conan Doyle"),
                       mutate(tidy_vh_books, author = "Victor Hugo")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Arthur Conan Doyle`:`Victor Hugo`)

frequency %>%
  ggplot()
```

