---
title: "Data Science on Unstructured Text Data - Quiz 2"
author: "Tamas Koncz"
date: '2018-02-19'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r, message=FALSE, include=FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)

library(dplyr)
library(ggplot2)
library(gridExtra)

knitr::opts_chunk$set(message = FALSE)

```


#### 1. Explain in your words what the unnest_token function does  
  
Given a data.frame, it splits up a define column into pieces - for our purpose of text mining, it will break a string vector into words. (Instead of words, it could also do ngrams, sentences etc.) The result will be organized in the long format, with not-impacted column data being replicated for each row.  
  

#### 2. Explain your words what the gutenbergr package does  

gutenbergr is an R API for the public domain collection of Project Gutenberg (~centralizing all books no longer under copyright restrictions). gutenbergr data also includes information of the works, and text is likely stripped of unncessary parts (e.g. page number).  

#### 3. Explain in your words how sentiment lexicon work  

Sentiment lexicons are just mapping files for words (or ngrams, combination of words) to a respective sentiment score. This score can be of different granularity - just positive/negative, or a number scale, or even categorical (e.g. "anger/joy/...").  

#### 4. How does inner_join provide sentiment analysis functionality  
  
We connect our text dataset (assuming it's already tidy) to the sentiment lexicon by inner_join - which will result in only the words that are in both datasets. Now, we have a sentiment score for all the remaining words in our text.  

#### 5. Explain in your words what tf-idf does  
  
TF-IDF creates a score for how important a word is in a document. Technically, it is based on weighting the word by the times it appears, divided by a term for it's general frequency. (So words that appear many times, but are very general, will get lower scores, while "special" word to the text will have higher scores)  

#### 6. Explain why you may want to do tokenization by bigram  
  
Sometimes looking at individual words could be misleading to understand the whole document - a good example is to think of negating word combinations, like "That was not smart", which might seem a positive statement if we score it only based on single words, as "smart" is likely scored positively.  

#### 7. Pick two or more authors that you are familiar with, download their texts using the gutenbergr package, and do a basic analysis of word frequencies and TF-IDF  
  
  
First step is to just get the ids of books for two authors:
```{r}
arthur_conan_doyle_ids <- gutenberg_works(author == "Doyle, Arthur Conan")[["gutenberg_id"]]

victor_hugo_ids <- gutenberg_works(author == "Hugo, Victor" & language == "en")[["gutenberg_id"]]
```
  
  
Then proceeding to download the respective books:
```{r}
arthur_conan_doyle <- gutenberg_download(arthur_conan_doyle_ids)
victor_hugo <- gutenberg_download(victor_hugo_ids)
```
  
  
In the next step two things are combined: unnesting the text column to words, and in the same step removing the useless ones (stopwords). Also adding a column, line number.
_Note: "de" was added to the stop words list based on its frequency in Hugo's works, as likely it just appears in French names without any added meaning_
```{r}
data(stop_words)
stop_words <- rbind(stop_words, data.frame(word = "de", lexicon = "custom"))

tidy_acd_books <- arthur_conan_doyle %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)


tidy_vh_books <- victor_hugo %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```
  
  
Let's visualize the most common words for both authors ( _note: I checked before, words we removed in the antijoin step earlier indeed looked like garbage for the purposes of text analysis_ )
```{r, fig.width=12, fig.height=6, fig.align='center'}
p1 <- tidy_acd_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 2500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() + 
  labs(title = "Most frequent words in the works of Arthur Conan Doyle", 
       subtitle = "Filtered for words appearing at least 2500 times,\nafter removing stopwords")

p2 <- tidy_vh_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 1250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() + 
  labs(title = "Most frequent words in the works of Victor Hugo", 
       subtitle = "Filtered for words appearing at least 1250 times,\nafter removing stopwords")

grid.arrange(p1, p2, ncol = 2)
```
  
    
Maybe not suprisingly, many of the most frequent words are shared among the two authors - think of "time", "eyes", etc. We'll explore commonalities further in next steps.   
  
```{r, fig.width=6, fig.height=5, fig.align='center'}
frequency <- bind_rows(mutate(tidy_acd_books, author = "Arthur Conan Doyle"),
                       mutate(tidy_vh_books, author = "Victor Hugo")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Arthur Conan Doyle`:`Victor Hugo`)

f_cor <- frequency %>%
  spread(key= author, value= proportion)

cor_est <- cor.test(x= f_cor$`Arthur Conan Doyle`, y= f_cor$`Victor Hugo`)$estimate

frequency %>%
  spread(key= author, value= proportion) %>%
  ggplot(aes(`Arthur Conan Doyle`, `Victor Hugo`)) + geom_point(alpha= .1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(title= "Comparing word frequencies between the two authors",
       subtitle= paste("Correlation strength:", round(cor_est, digits=4), sep=" ")) +
  geom_text(aes(label= word), check_overlap= TRUE)

```  
  
  
The above visualization of the joint-distribution of word frequencies between the two authors should not carry much surpirse either - many of the noted words above appear with similar frequencies.  
Some of the words where we see a big difference are related to either the British culture ("sir", "dear", "british") or to tematics of Victor Hugo's work ("paris", "jean", "child").  
Even if the results are not much news, it's still nice to see things quantified and visualized!  

The last analysis I'll perform is a comparison of these authors based on the sentiment of their works.  
  
  
  

Let's get started by creating a tidy dataset for both enhanced with sentiment scores from the AFFIN lexicon:  

```{r}
tidy_acd_books <- tidy_acd_books %>%
  inner_join(get_sentiments("afinn")) %>%
  mutate(author = "Arthur Conan Doyle")

tidy_vh_books <- tidy_vh_books %>%
  inner_join(get_sentiments("afinn")) %>%
  mutate(author = "Victor Hugo")
```


```{r, fig.width=12, fig.height=6, fig.align='center'}
p1 <- tidy_acd_books %>%
  mutate(total_count = n()) %>%
  group_by(word) %>% 
  summarise(sentiment = mean(score), 
            word_count = n(), 
            word_count_ratio = n() / mean(total_count)) %>%
  ungroup() %>%
  mutate(rank = dense_rank(desc(word_count))) %>%
  filter(rank <= 15) %>%
  mutate(word = reorder(word, word_count)) %>%
  ggplot(aes(word, sentiment * word_count_ratio, fill = sentiment >= 0)) + 
    geom_col(show.legend = FALSE) + 
    labs(title= "Most frequent words contributing to sentiment", 
         subtitle = "Arthur Conan Doyle",
         y = "Contribution to sentiment", x = NULL) + 
    coord_flip()

p2 <- tidy_vh_books %>%
  mutate(total_count = n()) %>%
  group_by(word) %>% 
  summarise(sentiment = mean(score), 
            word_count = n(), 
            word_count_ratio = n() / mean(total_count)) %>%
  ungroup() %>%
  mutate(rank = dense_rank(desc(word_count))) %>%
  filter(rank <= 15) %>%
  mutate(word = reorder(word, word_count)) %>%
  ggplot(aes(word, sentiment * word_count_ratio, fill = sentiment >= 0)) + 
    geom_col(show.legend = FALSE) + 
    labs(title= "Most frequent words contributing to sentiment", 
         subtitle = "Victor Hugo",
         y = "Contribution to sentiment", x = NULL) +
    coord_flip()

grid.arrange(p1, p2, ncol = 2)
```


For both authors, among most common words negative sentiment seems to be dominating - this is even true if we recognize that some words (like "dear") might not carry real sentiment, and are rather just part of common language used by the author.  
Although my knowledge of these works are limited, based on the few books I read in the collection I believe the above make sense - for example, 'Les Miserables' is dominated by fighting against all odds, and trying to hang on to hope.  
  
Talking of hope - an important aspect of sentiment is not just its aggregate, but how it builds up across a book! 

```{r, fig.width=12, fig.height=10, fig.align='center'}
p1 <- tidy_acd_books %>%
  filter(gutenberg_id %in% 1661) %>%
  mutate(linenumber = row_number()) %>%
  group_by(author, index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>%
  ggplot(aes(x=index, y = sentiment, fill = sentiment >= 0)) +
    geom_bar(stat="identity") +
    labs(title="Sentiment dynamics in 'The Adventures of Sherlock Holmes'") + 
    theme(legend.position="none")

p2 <- tidy_vh_books %>%
  filter(gutenberg_id %in% 48731:48735) %>% ##the books is divided to five parts
  mutate(linenumber = row_number()) %>%
  group_by(author, index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>%
  ggplot(aes(x=index, y = sentiment, fill = sentiment >= 0)) +
    geom_bar(stat="identity") +
    labs(title="Sentiment dynamics in 'Les Miserables'") + 
    theme(legend.position="none")
  
grid.arrange(p1, p2, ncol=1)
```

Above I plotted aggregate sentiment scores across maybe the two most notable books by the authors.  
What is interesting that both are carrying the negative sentiment across most of the works, with some spikes of positivity.  
Does this make sense?  
- For 'Les Miserables', I certainly think so. Let's consider the ending, which is mostly positive but there is a negative part as well. In the book, Jean Valjean dies at the end, preceded by a happy wedding. Even though the ending is "happy", there is still a lot of drama involved.
- 'The Adventures of Sherlock Holmes' is actually a collection of shorter stories, which definitely has some impact on general dynamics. The stories are also written from a special perspective (Watson's). Although I think painting these stories as being dominated by negative sentiment is misleading, I can understand why we see these results - the topic is focused around crime investigation, social injustice, etc.  
  
What is my take-away from this? In one sentence: if we are after analyzing sentiments of a very (narrowly) thematical writing, we better be prepared with a specialized sentiment lexicon as well.  
  
Now that we understand that the works of A.C.D. and V.Hugo share some shared characteristic on the high level (most common words, sentiments), it's time to look at the differences.  
A great way of achieving this is by TF-IDF analysis, which should help us find the 'most descriptive' words for each document.

